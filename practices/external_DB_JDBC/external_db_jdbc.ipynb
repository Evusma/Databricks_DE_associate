{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "122b139f-9ddb-41e3-bf6d-778065d7709e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The tables to create are listed on a csv. This csv must be uploaded in the Volume or the DBFS.\n",
    "\n",
    "If you want to use a csv file with Spark, you should upload it to DBFS or to the volum. **Having the file in the same worksapce directory as your notebook is not sufficient.**\n",
    "\n",
    "To see the files uploaded in DBFS: \n",
    "`display(dbutils.fs.ls(\"dbfs:/FileStore/\"))` The dbutils.fs.ls command only lists files in DBFS, not workspace files. To access workspace files, you should use shell commands such as %sh ls instead.\n",
    "\n",
    "To list files in the workspace folder: \n",
    "```\n",
    "%sh\n",
    "ls\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90bd67b-211a-4eaf-a55c-247b5f1f8e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled Notebook 2025-10-06 11:54:07.ipynb\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9907354f-9b70-4bc7-9d56-02e8be6e220c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, we have the list of tables in a csv which has been uploaded in pur volum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cace87-be3d-4e6e-b091-3c7c771e2019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the information using a spark dataframe\n",
    "df = spark.read.option(\"header\",\"true\").csv('/Volumes/data_2025-10-06.csv')\n",
    "df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860829db-573e-45f2-84fd-939f0be71b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nom_schema: string (nullable = true)\n",
      " |-- nom_table: string (nullable = true)\n",
      " |-- info_table: string (nullable = true)\n",
      " |-- type_table: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- nombre_colonnes: string (nullable = true)\n",
      " |-- nombre_lignes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the csv\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47bb24c3-601b-470f-997d-df118ee327f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a list of the name of schemas and a list of the name of tables, both from the spark dataframe\n",
    "schemas = [row['nom_schema'] for row in df.select('nom_schema').collect()]\n",
    "tables = [row['nom_table'] for row in df.select('nom_table').collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "654b1c0e-2ab5-44ba-8cc1-ad937c894db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the catalog in Databricks\n",
    "spark.sql(\"\"\"\n",
    "    CREATE CATALOG IF NOT EXISTS data\n",
    "    USING 'jdbc'\n",
    "    OPTIONS (\n",
    "        url 'jdbc:postgresql://<hostname>:<port>/<database>',\n",
    "        user '<username>',\n",
    "        password '<password>',\n",
    "        driver 'org.postgresql.Driver'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "808247d2-e693-4981-917e-93bab9477298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following approach registers the table in the Databricks catalog and queries will be pushed down to PostgreSQL, so data is not ingested into Databricks storage:\n",
    "\n",
    "(This requires Unity Catalog and the JDBC table feature enabled in your workspace.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8981491-8fe2-428e-abbd-6cd0e31716ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the table in the Databricks catalog\n",
    "for schema, table in zip(schemas, tables):\n",
    "    table = f\"{schema}.{table}\"\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS datageo.{table}\n",
    "    USING JDBC\n",
    "    OPTIONS (\n",
    "        url 'jdbc:postgresql:://<hostname>:<port>/<database>',\n",
    "        dbtable '{table}',\n",
    "        user '<username>',\n",
    "        password '<password>',\n",
    "        driver 'org.postgresql.Driver'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4161af-7f0c-4435-a033-3c21d1e771a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The JDBC table feature in Databricks Unity Catalog allows you to register external tables that reference data in JDBC-accessible databases (like PostgreSQL) without ingesting the data into Databricks storage. This feature must be enabled in your workspace to use SQL statements like `CREATE TABLE ... USING JDBC ...` that register external tables in Unity Catalog and push queries down to the source database.\n",
    "\n",
    "To check or enable this feature, you need to:\n",
    "\n",
    "- Be on a supported Databricks Runtime.\n",
    "- Have Unity Catalog enabled in your workspace.\n",
    "- Have the JDBC table feature enabled by your Databricks admin.\n",
    "\n",
    "This is a workspace-level setting that may require admin action. If you do not see errors when running `CREATE TABLE ... USING JDBC ...`, the feature is likely enabled.\n",
    "\n",
    "If you are unsure, contact your Databricks workspace admin to confirm that the JDBC table feature is enabled for Unity Catalog in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af50e186-cf68-4580-91cd-0ab0b8a18dc1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":429},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759754698511}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4521169634053176,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "script_tables_datageo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
