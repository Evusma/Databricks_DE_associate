{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9accf121-4e12-4d65-a02d-e92b17e4c1d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), True),\n",
    "   StructField(\"name\", StringType(), True),\n",
    "   StructField(\"age\", IntegerType(), True),\n",
    "   StructField(\"money\", IntegerType(), True),\n",
    "   StructField(\"sales\", IntegerType(), True),\n",
    "   StructField(\"units\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# --------------------------\n",
    "# Bronze: Streaming ingestion\n",
    "# --------------------------\n",
    "@dp.table\n",
    "def bronze_sales_spark():\n",
    "    \"\"\"\n",
    "    Ingest raw JSON sales data continuously\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .option(\"multiLine\", True)\n",
    "        .schema(schema)\n",
    "        .load(\"/Volumes/dataops_dev/schema_test/volume_test/delta_tables/json/\")\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# Silver: Batch clean & quality\n",
    "# --------------------------\n",
    "@dp.table\n",
    "@dp.expect(\"valid_amount\", \"money > 0\")  # data quality check\n",
    "def silver_sales_spark():\n",
    "    \"\"\"\n",
    "    Clean data: cast types, enforce quality rules\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"bronze_sales_spark\")\n",
    "        .withColumn(\"money\", col(\"money\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# Gold: Batch aggregation\n",
    "# --------------------------\n",
    "@dp.table\n",
    "def gold_sales_summary_spark():\n",
    "    \"\"\"\n",
    "    Aggregate silver data in batch mode.\n",
    "    You can run this on schedule or manually.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read(\"silver_sales_spark\")\n",
    "        .groupBy(\"Id\")\n",
    "        .agg(\n",
    "            _sum(\"money\").alias(\"total_money\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# This notebook is a pipeline definition. It is the source code for the pipeline. \n",
    "# The source code is added on the pipeline definition page or pipeline settings.\n",
    "# On the pipeline settings you define the mode (triggered or continuous), the compute, the destination (catalog and schema)\n",
    "# On the pipeline page, you can see the graph, the state, the logs, the schedule (and here the notifications)\n",
    "# --------------------------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5190277786092196,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DP-pyspark 2025-10-01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
